{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/rabbitmetrics/langchain-13-min/blob/main/notebooks/langchain-13-min.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "50dvxjqCFmhF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables\n",
    "from dotenv import load_dotenv,find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "C7RnyUOCJWmk"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nLarge language models are like an encyclopedia for words. They help computers understand how the words in a language fit together and how they should be used. They help computers learn to think like humans, so they can understand the things we say and write.'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run basic query with OpenAI wrapper\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "llm = OpenAI(model_name=\"text-davinci-003\")\n",
    "llm(\"explain large language models to a child.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat LLM Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "JtHgQ5XpJmgi"
   },
   "outputs": [],
   "source": [
    "# import schema for chat messages and ChatOpenAI in order to query chatmodels GPT-3.5-turbo or GPT-4\n",
    "\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yrfYfKfdJyyF",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's an example script that trains a simple neural network on simulated data using PyTorch:\n",
      "\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "import numpy as np\n",
      "\n",
      "# Define the neural network architecture\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(2, 10)\n",
      "        self.fc2 = nn.Linear(10, 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# Generate some simulated data\n",
      "x_train = np.random.rand(100, 2)\n",
      "y_train = np.sum(x_train, axis=1, keepdims=True)\n",
      "\n",
      "# Convert the data to PyTorch tensors\n",
      "x_train = torch.from_numpy(x_train).float()\n",
      "y_train = torch.from_numpy(y_train).float()\n",
      "\n",
      "# Initialize the neural network and the optimizer\n",
      "net = Net()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
      "\n",
      "# Train the neural network\n",
      "for epoch in range(100):\n",
      "    optimizer.zero_grad()\n",
      "    y_pred = net(x_train)\n",
      "    loss = nn.MSELoss()(y_pred, y_train)\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "    print('Epoch %d, Loss: %.4f' % (epoch+1, loss.item()))\n",
      "\n",
      "# Test the neural network on some new data\n",
      "x_test = torch.tensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
      "y_test = net(x_test)\n",
      "print('Test Predictions:', y_test)\n",
      "```\n",
      "\n",
      "In this example, we define a simple neural network with two hidden layers and train it on a simulated dataset where the output is the sum of the inputs. We use the mean squared error loss function and stochastic gradient descent optimizer to train the network. Finally, we test the network on some new data.\n"
     ]
    }
   ],
   "source": [
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an expert data scientist\"),\n",
    "    HumanMessage(content=\"Write a Python script that trains a neural network on simulated data using pytorch.\")\n",
    "]\n",
    "response=chat(messages)\n",
    "\n",
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and ChatLLM Prompting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2grf7I8AJ_hK"
   },
   "outputs": [],
   "source": [
    "# Import prompt and define PromptTemplate\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "template = \"\"\"\n",
    "You are an expert physicist with an expertise in theoretical physics. \n",
    "Mathematically derive {concept} . \n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vcz7Q9Y-KFvI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nTime dilation in special relativity is derived from two postulates: the principle of relativity and the constancy of the speed of light.\\n\\nThe principle of relativity states that the laws of physics are the same in all inertial frames. In other words, laws of physics should look the same in any inertial frame of reference regardless of the motion of that frame.\\n\\nThe constancy of the speed of light states that the speed of light in a vacuum is always the same, no matter the motion of the observer or the source. \\n\\nThese two postulates combined allow us to derive the time dilation equation. \\n\\nLet's assume two frames of reference, S and S'. The distance between two events A and B is $\\\\Delta x$, and the time elapsed between them is $\\\\Delta t$. The time elapsed in frame S' between the same two events is $\\\\Delta t'$. We can relate these two times by the Lorentz transformation.\\n\\n$$\\\\Delta t' = \\\\gamma(\\\\Delta t - \\\\frac{v\\\\Delta x}{c^2})$$\\n\\nwhere $\\\\gamma$ is the Lorentz factor, and $v$ is the velocity of S' relative to S. \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run LLM with PromptTemplate\n",
    "\n",
    "llm(prompt.format(concept=\"time dilation in special relativity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's an example script that trains a simple neural network on simulated data using PyTorch:\n",
      "\n",
      "```\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "import torch.optim as optim\n",
      "import numpy as np\n",
      "\n",
      "# Define the neural network architecture\n",
      "class Net(nn.Module):\n",
      "    def __init__(self):\n",
      "        super(Net, self).__init__()\n",
      "        self.fc1 = nn.Linear(2, 10)\n",
      "        self.fc2 = nn.Linear(10, 1)\n",
      "\n",
      "    def forward(self, x):\n",
      "        x = torch.relu(self.fc1(x))\n",
      "        x = self.fc2(x)\n",
      "        return x\n",
      "\n",
      "# Generate some simulated data\n",
      "X = np.random.rand(100, 2)\n",
      "y = np.sum(X, axis=1).reshape(-1, 1)\n",
      "\n",
      "# Convert the data to PyTorch tensors\n",
      "X = torch.from_numpy(X).float()\n",
      "y = torch.from_numpy(y).float()\n",
      "\n",
      "# Initialize the neural network and the optimizer\n",
      "net = Net()\n",
      "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
      "\n",
      "# Train the neural network\n",
      "for epoch in range(1000):\n",
      "    optimizer.zero_grad()\n",
      "    output = net(X)\n",
      "    loss = nn.MSELoss()(output, y)\n",
      "    loss.backward()\n",
      "    optimizer.step()\n",
      "\n",
      "    if epoch % 100 == 0:\n",
      "        print(f\"Epoch {epoch}, loss {loss.item()}\")\n",
      "\n",
      "# Test the neural network on some new data\n",
      "X_test = torch.tensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]])\n",
      "y_pred = net(X_test)\n",
      "print(y_pred)\n",
      "```\n",
      "\n",
      "This script defines a simple neural network with two input neurons, one hidden layer with ten neurons, and one output neuron. It generates some simulated data where the output is the sum of the input features, and trains the neural network to predict the output from the input features. Finally, it tests the neural network on some new data.\n"
     ]
    }
   ],
   "source": [
    "# Run chatopenai with PromptTemplate\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
    "\n",
    "\n",
    "template2 = \"\"\"\n",
    "Mathematically derive {concept} . \n",
    "\"\"\"\n",
    "\n",
    "prompt2 = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "\n",
    "messages2 = [\n",
    "    SystemMessage(content=\"You are an expert physicist with an expertise in theoretical physics\"),\n",
    "    HumanMessage(content=prompt.format(concept=\"time dilation in special relativity\") )\n",
    "]\n",
    "response=chat(messages)\n",
    "\n",
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "dm78i-rUKXIB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The Lorentz transformation is a mathematical equation that describes the transformation of space and time between two inertial frames of reference in special relativity. It is given by the following equation: \n",
      "\n",
      "x' = γ(x - vt) \n",
      "\n",
      "t' = γ(t - vx/c²) \n",
      "\n",
      "where x is the position coordinate, t is the time coordinate, v is the relative velocity between the two frames, c is the speed of light, and γ is the Lorentz factor. \n",
      "\n",
      "The Lorentz factor γ is given by: \n",
      "\n",
      "γ = 1/√(1 - (v²/c²)) \n",
      "\n",
      "Using the above equation, we can then derive the Lorentz transformation as follows: \n",
      "\n",
      "x' = (1/√(1-(v²/c²)))(x - vt) \n",
      "\n",
      "t' = (1/√(1-(v²/c²)))(t - (v/c²)x)\n"
     ]
    }
   ],
   "source": [
    "# Import LLMChain and define chain with language model and prompt as arguments.\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "print(chain.run(\"lorentz transformation\")) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "B6MF4-nMKul3"
   },
   "outputs": [],
   "source": [
    "# Define a second prompt \n",
    "\n",
    "second_prompt = PromptTemplate(\n",
    "    input_variables=[\"concept\"],\n",
    "    template=\"Turn the concept description of {concept} and explain it to me like I'm five. \\\n",
    "    Provide complete responses. \",\n",
    ")\n",
    "chain_two = LLMChain(llm=llm, prompt=second_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "SkJKFyk1K-MO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
      "\u001b[36;1m\u001b[1;3m\n",
      "The Lorentz transformation is a mathematical transformation between two coordinate systems S and S' which are in uniform relative motion with respect to each other.\n",
      "\n",
      "Let x,y,z be the coordinates of a point in S and x',y',z' be the coordinates of the same point in S'. Let v be the uniform velocity of S' relative to S. \n",
      "\n",
      "The Lorentz Transformation is given by:\n",
      "\n",
      "x' = γ(x - vt) \n",
      "y' = y \n",
      "z' = z \n",
      "\n",
      "where γ = (1 - v2/c2)−1/2\n",
      "\n",
      "Let t' = t be the coordinates of a point in S and t'' = t' be the coordinates of the same point in S'. \n",
      "\n",
      "Then the Lorentz Transformation can be written as: \n",
      "\n",
      "t'' = γ(t - vx/c2) \n",
      "x'' = γ(x - vt) \n",
      "y'' = y \n",
      "z'' = z\u001b[0m\n",
      "\u001b[33;1m\u001b[1;3m\n",
      "\n",
      "The Lorentz Transformation is a way of figuring out where something is in two different places at the same time. Imagine you are standing in one place and then you move to another place. The Lorentz Transformation helps you figure out where you are in both places at the same time. \n",
      "\n",
      "To use the Lorentz Transformation, you need to know two things: a velocity, which is how fast something is moving, and a speed of light, which is how quickly light travels. \n",
      "\n",
      "Then, you can use the equation to figure out how far you have moved, and in what direction. The equation looks like this: \n",
      "\n",
      "x' = γ(x - vt) \n",
      "y' = y \n",
      "z' = z \n",
      "\n",
      "where γ = (1 - v2/c2)−1/2\n",
      "\n",
      "This equation tells you the position of an object in two different places at the same time. The x' tells you the position of the object in one place, and the y' and z' tell you the position of the object in the other place. \n",
      "\n",
      "So, if you are standing in one place and then move to another, the Lorentz Transformation can help\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "The Lorentz Transformation is a way of figuring out where something is in two different places at the same time. Imagine you are standing in one place and then you move to another place. The Lorentz Transformation helps you figure out where you are in both places at the same time. \n",
      "\n",
      "To use the Lorentz Transformation, you need to know two things: a velocity, which is how fast something is moving, and a speed of light, which is how quickly light travels. \n",
      "\n",
      "Then, you can use the equation to figure out how far you have moved, and in what direction. The equation looks like this: \n",
      "\n",
      "x' = γ(x - vt) \n",
      "y' = y \n",
      "z' = z \n",
      "\n",
      "where γ = (1 - v2/c2)−1/2\n",
      "\n",
      "This equation tells you the position of an object in two different places at the same time. The x' tells you the position of the object in one place, and the y' and z' tell you the position of the object in the other place. \n",
      "\n",
      "So, if you are standing in one place and then move to another, the Lorentz Transformation can help\n"
     ]
    }
   ],
   "source": [
    "# Define a sequential chain using the two chains above: the second chain takes the output of the first chain as input\n",
    "\n",
    "from langchain.chains import SimpleSequentialChain\n",
    "overall_chain = SimpleSequentialChain(chains=[chain, chain_two], verbose=True)\n",
    "\n",
    "# Run the chain specifying only the input variable for the first chain.\n",
    "explanation = overall_chain.run(\"lorentz transformation\")\n",
    "print(explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'model name': 'Linear Regression', 'assumption': 'One commonly used metric to evaluate the performance of a Linear Regression model is the Root Mean Squared Error (RMSE). \\n\\nRMSE is a measure of how well the model is able to predict the outcome variable (also known as the dependent variable) based on the input variables (also known as the independent variables). It is calculated by taking the square root of the average of the squared differences between the predicted values and the actual values of the outcome variable.\\n\\nTo explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their characteristics. \\n\\nTo evaluate how well your model is able to predict the actual prices of houses, you can use the RMSE metric. The RMSE tells you how far off your predicted prices are from the actual prices, on average. \\n\\nFor example, if the RMSE is $10,000, it means that on average, your predicted prices are off by $10,000 from the actual prices. A lower RMSE indicates that your model is better at predicting prices, while a higher RMSE indicates that your model is not as accurate.', 'assumption2': 'Another metric that can be used to evaluate the performance of a Linear Regression model is the R-squared (R2) value. \\n\\nR2 is a measure of how well the model fits the data. It represents the proportion of the variance in the outcome variable that can be explained by the input variables. \\n\\nTo explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their characteristics. \\n\\nTo evaluate how well your model fits the data, you can use the R2 metric. The R2 tells you how much of the variation in house prices can be explained by the size, number of bedrooms, and location of the house. \\n\\nFor example, if the R2 is 0.8, it means that 80% of the variation in house prices can be explained by the size, number of bedrooms, and location of the house. A higher R2 indicates that your model is better at fitting the data, while a lower R2 indicates that your model is not as good at explaining the variation in house prices.'}\n"
     ]
    }
   ],
   "source": [
    "# Import LLMChain and define chain with language model and prompt as arguments.\n",
    "\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    PromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    AIMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "\n",
    "# ChatModel\n",
    "chat = ChatOpenAI(model_name=\"gpt-3.5-turbo\",temperature=0.3)\n",
    "\n",
    "\n",
    "# Define system prompts and human prompt for chat model\n",
    "system_prompt1 = PromptTemplate(\n",
    "    template = \"You are a data scientist with expertise in regression and statistics. Please provide answer to following questions. \",\n",
    "    input_variables=[]\n",
    ")\n",
    "\n",
    "human_prompt1 = PromptTemplate(template=\"Randomly pick one metric to evaluate performance of {model name} model \\\n",
    "                                         and explain it to a non-technical user.\\\n",
    "                                         Assumption:  \", \n",
    "                               input_variables=[\"model name\"])\n",
    "\n",
    "system_message_prompt1 = SystemMessagePromptTemplate(prompt=system_prompt1)\n",
    "human_message_prompt1 = HumanMessagePromptTemplate(prompt=human_prompt1)\n",
    "\n",
    "messages1 = [system_message_prompt1, human_message_prompt1]\n",
    "\n",
    "# Covert messages into chat prompts template\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages(messages1)\n",
    "\n",
    "# Create LLM Chain\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template, output_key=\"assumption\")\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "#print(chain.run({\"variable\":\"rainfall in mm\", \"plant\":\"rose\"})) \n",
    "\n",
    "#Second Prompt\n",
    "human_prompt2 = PromptTemplate(template=\"Randomly pick one metric to evaluate performance of this model except {assumption} \\\n",
    "                                         and explain it to a non-technical user.\\\n",
    "                                         Assumption 2 = \", \n",
    "                               input_variables=[\"assumption\"])\n",
    "\n",
    "human_message_prompt2 = HumanMessagePromptTemplate(prompt=human_prompt2)\n",
    "\n",
    "messages2 = [ human_message_prompt2]\n",
    "\n",
    "# Covert messages into chat prompts template\n",
    "chat_prompt_template2 = ChatPromptTemplate.from_messages(messages2)\n",
    "\n",
    "# Create LLM Chain\n",
    "chain_two = LLMChain(llm=chat, prompt=chat_prompt_template2, output_key=\"assumption2\")\n",
    "\n",
    "overall_chain = SequentialChain(chains=[chain, chain_two],\n",
    "                                      input_variables=[\"model name\"],\n",
    "                                      output_variables=[\"assumption\", \"assumption2\"],\n",
    "                                      verbose=True)\n",
    "explanation = overall_chain({\"model name\":\"Linear Regression\"})\n",
    "print(explanation)\n",
    "\n",
    "# Run the chain only specifying the input variable.\n",
    "#print(chain.run()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model name': 'Linear Regression',\n",
       " 'assumption': 'One commonly used metric to evaluate the performance of a Linear Regression model is the Root Mean Squared Error (RMSE). \\n\\nRMSE is a measure of how well the model is able to predict the outcome variable (also known as the dependent variable) based on the input variables (also known as the independent variables). It is calculated by taking the square root of the average of the squared differences between the predicted values and the actual values of the outcome variable.\\n\\nTo explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their characteristics. \\n\\nTo evaluate how well your model is able to predict the actual prices of houses, you can use the RMSE metric. The RMSE tells you how far off your predicted prices are from the actual prices, on average. \\n\\nFor example, if the RMSE is $10,000, it means that on average, your predicted prices are off by $10,000 from the actual prices. A lower RMSE indicates that your model is better at predicting prices, while a higher RMSE indicates that your model is not as accurate.',\n",
       " 'assumption2': 'Another metric that can be used to evaluate the performance of a Linear Regression model is the R-squared (R2) value. \\n\\nR2 is a measure of how well the model fits the data. It represents the proportion of the variance in the outcome variable that can be explained by the input variables. \\n\\nTo explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their characteristics. \\n\\nTo evaluate how well your model fits the data, you can use the R2 metric. The R2 tells you how much of the variation in house prices can be explained by the size, number of bedrooms, and location of the house. \\n\\nFor example, if the R2 is 0.8, it means that 80% of the variation in house prices can be explained by the size, number of bedrooms, and location of the house. A higher R2 indicates that your model is better at fitting the data, while a lower R2 indicates that your model is not as good at explaining the variation in house prices.'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "mDDu1B_SLQls"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='One commonly used metric to evaluate the performance of a Linear Regression model is the Root Mean Squared Error (RMSE).', metadata={}),\n",
       " Document(page_content='RMSE is a measure of how well the model is able to predict the outcome variable (also known as the dependent variable) based on the input variables (also known as the independent variables). It is calculated by taking the square root of the average', metadata={}),\n",
       " Document(page_content='of the squared differences between the predicted values and the actual values of the outcome variable.', metadata={}),\n",
       " Document(page_content='To explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their', metadata={}),\n",
       " Document(page_content='characteristics.', metadata={}),\n",
       " Document(page_content='To evaluate how well your model is able to predict the actual prices of houses, you can use the RMSE metric. The RMSE tells you how far off your predicted prices are from the actual prices, on average.', metadata={}),\n",
       " Document(page_content='For example, if the RMSE is $10,000, it means that on average, your predicted prices are off by $10,000 from the actual prices. A lower RMSE indicates that your model is better at predicting prices, while a higher RMSE indicates that your model is', metadata={}),\n",
       " Document(page_content='not as accurate.Another metric that can be used to evaluate the performance of a Linear Regression model is the R-squared (R2) value.', metadata={}),\n",
       " Document(page_content='R2 is a measure of how well the model fits the data. It represents the proportion of the variance in the outcome variable that can be explained by the input variables.', metadata={}),\n",
       " Document(page_content='To explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their', metadata={}),\n",
       " Document(page_content='characteristics.', metadata={}),\n",
       " Document(page_content='To evaluate how well your model fits the data, you can use the R2 metric. The R2 tells you how much of the variation in house prices can be explained by the size, number of bedrooms, and location of the house.', metadata={}),\n",
       " Document(page_content='For example, if the R2 is 0.8, it means that 80% of the variation in house prices can be explained by the size, number of bedrooms, and location of the house. A higher R2 indicates that your model is better at fitting the data, while a lower R2', metadata={}),\n",
       " Document(page_content='indicates that your model is not as good at explaining the variation in house prices.', metadata={})]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import utility for splitting up texts and split up the explanation given above into document chunks\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 250,\n",
    "    chunk_overlap  = 0,\n",
    ")\n",
    "\n",
    "texts = text_splitter.create_documents([explanation['assumption'] + explanation['assumption2']])\n",
    "\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "F6lfAdeuLhtp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One commonly used metric to evaluate the performance of a Linear Regression model is the Root Mean Squared Error (RMSE).\n",
      "RMSE is a measure of how well the model is able to predict the outcome variable (also known as the dependent variable) based on the input variables (also known as the independent variables). It is calculated by taking the square root of the average\n",
      "of the squared differences between the predicted values and the actual values of the outcome variable.\n",
      "To explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their\n",
      "characteristics.\n",
      "To evaluate how well your model is able to predict the actual prices of houses, you can use the RMSE metric. The RMSE tells you how far off your predicted prices are from the actual prices, on average.\n",
      "For example, if the RMSE is $10,000, it means that on average, your predicted prices are off by $10,000 from the actual prices. A lower RMSE indicates that your model is better at predicting prices, while a higher RMSE indicates that your model is\n",
      "not as accurate.Another metric that can be used to evaluate the performance of a Linear Regression model is the R-squared (R2) value.\n",
      "R2 is a measure of how well the model fits the data. It represents the proportion of the variance in the outcome variable that can be explained by the input variables.\n",
      "To explain this to a non-technical user, imagine that you are trying to predict the price of a house based on its size, number of bedrooms, and location. You build a Linear Regression model using historical data on house prices and their\n",
      "characteristics.\n",
      "To evaluate how well your model fits the data, you can use the R2 metric. The R2 tells you how much of the variation in house prices can be explained by the size, number of bedrooms, and location of the house.\n",
      "For example, if the R2 is 0.8, it means that 80% of the variation in house prices can be explained by the size, number of bedrooms, and location of the house. A higher R2 indicates that your model is better at fitting the data, while a lower R2\n",
      "indicates that your model is not as good at explaining the variation in house prices.\n"
     ]
    }
   ],
   "source": [
    "# Individual text chunks can be accessed with \"page_content\"\n",
    "for i in range(len(texts)):\n",
    "    print(texts[i].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Z5sv4e3tLw2y"
   },
   "outputs": [],
   "source": [
    "# Import and instantiate OpenAI embeddings\n",
    "\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model_name=\"ada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dqzoir4hMlfl"
   },
   "outputs": [],
   "source": [
    "# Turn the first text chunk into a vector with the embedding\n",
    "import numpy as np\n",
    "\n",
    "query_result = embeddings.embed_query(texts[i].page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "QaOY5bIZM3Xz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bhaveshkhamesra/anaconda3/envs/env_llm/lib/python3.8/site-packages/pinecone/index.py:4: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm\n",
      "Using embedded DuckDB without persistence: data will be transient\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Document' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvectorstores\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Chroma\n\u001b[1;32m      6\u001b[0m index_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain-quickstart\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 8\u001b[0m docsearch \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#pinecone.init(\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#    api_key=os.getenv('PINECONE_API_KEY'),  \u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m#    environment=os.getenv('PINECONE_ENV')  \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m#index_name = \"langchain-quickstart\"\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m#search = Pinecone.from_documents(texts, embeddings, index_name=index_name)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat can Lorentz transformation tell us?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/env_llm/lib/python3.8/site-packages/langchain/vectorstores/chroma.py:305\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, **kwargs)\u001b[0m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;03m\"\"\"Create a Chroma vectorstore from a raw documents.\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;03mIf a persist_directory is specified, the collection will be persisted there.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;03m    Chroma: Chroma vectorstore.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    299\u001b[0m chroma_collection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m    300\u001b[0m     collection_name\u001b[38;5;241m=\u001b[39mcollection_name,\n\u001b[1;32m    301\u001b[0m     embedding_function\u001b[38;5;241m=\u001b[39membedding,\n\u001b[1;32m    302\u001b[0m     persist_directory\u001b[38;5;241m=\u001b[39mpersist_directory,\n\u001b[1;32m    303\u001b[0m     client_settings\u001b[38;5;241m=\u001b[39mclient_settings,\n\u001b[1;32m    304\u001b[0m )\n\u001b[0;32m--> 305\u001b[0m \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/anaconda3/envs/env_llm/lib/python3.8/site-packages/langchain/vectorstores/chroma.py:115\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_embedding_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 115\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_collection\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    117\u001b[0m     metadatas\u001b[38;5;241m=\u001b[39mmetadatas, embeddings\u001b[38;5;241m=\u001b[39membeddings, documents\u001b[38;5;241m=\u001b[39mtexts, ids\u001b[38;5;241m=\u001b[39mids\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ids\n",
      "File \u001b[0;32m~/anaconda3/envs/env_llm/lib/python3.8/site-packages/langchain/embeddings/openai.py:275\u001b[0m, in \u001b[0;36mOpenAIEmbeddings.embed_documents\u001b[0;34m(self, texts, chunk_size)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# handle batches of large input text\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ctx_length \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_len_safe_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_model_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    277\u001b[0m     results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/env_llm/lib/python3.8/site-packages/langchain/embeddings/openai.py:209\u001b[0m, in \u001b[0;36mOpenAIEmbeddings._get_len_safe_embeddings\u001b[0;34m(self, texts, engine, chunk_size)\u001b[0m\n\u001b[1;32m    206\u001b[0m encoding \u001b[38;5;241m=\u001b[39m tiktoken\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mencoding_for_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdocument_model_name)\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, text \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(texts):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# replace newlines, which can negatively affect performance.\u001b[39;00m\n\u001b[0;32m--> 209\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplace\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    210\u001b[0m     token \u001b[38;5;241m=\u001b[39m encoding\u001b[38;5;241m.\u001b[39mencode(\n\u001b[1;32m    211\u001b[0m         text,\n\u001b[1;32m    212\u001b[0m         allowed_special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mallowed_special,\n\u001b[1;32m    213\u001b[0m         disallowed_special\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisallowed_special,\n\u001b[1;32m    214\u001b[0m     )\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(token), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_ctx_length):\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Document' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "# Import and initialize Pinecone client\n",
    "\n",
    "import os\n",
    "import pinecone\n",
    "from langchain.vectorstores import Chroma\n",
    "index_name = \"langchain-quickstart\"\n",
    "\n",
    "docsearch = Chroma.from_texts(texts, embeddings)\n",
    "#pinecone.init(\n",
    "#    api_key=os.getenv('PINECONE_API_KEY'),  \n",
    "#    environment=os.getenv('PINECONE_ENV')  \n",
    "#)\n",
    "\n",
    "# Upload vectors to Pinecone\n",
    "\n",
    "#index_name = \"langchain-quickstart\"\n",
    "#search = Pinecone.from_documents(texts, embeddings, index_name=index_name)\n",
    "\n",
    "query = \"What can Lorentz transformation tell us?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lZhSUt3FNBzN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCXVuXwPNKcc"
   },
   "outputs": [],
   "source": [
    "# Do a simple vector similarity search\n",
    "\n",
    "query = \"What is magical about an autoencoder?\"\n",
    "result = search.similarity_search(query)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4Ogz8luZNRnJ"
   },
   "outputs": [],
   "source": [
    "# Import Python REPL tool and instantiate Python agent\n",
    "\n",
    "from langchain.agents.agent_toolkits import create_python_agent\n",
    "from langchain.tools.python.tool import PythonREPLTool\n",
    "from langchain.python import PythonREPL\n",
    "from langchain.llms.openai import OpenAI\n",
    "\n",
    "agent_executor = create_python_agent(\n",
    "    llm=OpenAI(temperature=0, max_tokens=1000),\n",
    "    tool=PythonREPLTool(),\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BVHMDj0sNi09"
   },
   "outputs": [],
   "source": [
    "# Execute the Python agent\n",
    "\n",
    "agent_executor.run(\"Find the roots (zeros) if the quadratic function 3 * x**2 + 2*x -1\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONM96f7/m0jUCD9c87+MQy",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "llm_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
